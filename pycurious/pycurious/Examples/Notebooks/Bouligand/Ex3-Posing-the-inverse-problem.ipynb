{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3 - Posing the Inverse Problem\n",
    "\n",
    "It is notoriously difficult to estimate uncertainties from Curie depth determinations. Intuitively, the fewer points in the power spectrum should result in higher uncertainty, but it is difficult to quantify these uncertainties in practise. For [Bouligand *et al.*, 2009](http://doi.wiley.com/10.1029/2009JB006494) it is difficult to determine the values of $\\beta$ and $\\Delta z$ since both control the slope of the power spectrum at low wavenumbers. Similarly, for [Tanaka *et al.*, 1999](http://linkinghub.elsevier.com/retrieve/pii/S0040195199000724) the lower and upper ranges of the power spectrum used to compute $z_b$ and $z_t$ is highly subjective and can result in significantly different Curie depths.\n",
    "\n",
    "Here, we pose the problem of finding the Curie depth from the radial power spectrum within a Bayesian framework. Thus, we can effectively estimate the uncertainty of our Curie depth estimates.\n",
    "\n",
    "### Contents\n",
    "\n",
    "- [Bayesian inverse framework](#Bayesian-inverse-framework)\n",
    "- [Metropolis-Hastings algorithm](#Metropolis-Hastings-algorithm)\n",
    "- [Appraising the ensemble](#Appraising-the-ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "import pycurious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x,y,anomaly\n",
    "mag_data = np.loadtxt(\"../../data/test_mag_data.txt\")\n",
    "\n",
    "nx, ny = 305, 305\n",
    "\n",
    "x = mag_data[:,0]\n",
    "y = mag_data[:,1]\n",
    "d = mag_data[:,2].reshape(ny,nx)\n",
    "\n",
    "xmin, xmax = x.min(), x.max()\n",
    "ymin, ymax = y.min(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pycurious.CurieOptimise(d, xmin, xmax, ymin, ymax)\n",
    "\n",
    "# pick the centroid\n",
    "xpt = 0.5*(xmin + xmax)\n",
    "ypt = 0.5*(ymin + ymax)\n",
    "\n",
    "window_size = 200e3\n",
    "subgrid = grid.subgrid(window_size, xpt, ypt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inverse framwork\n",
    "\n",
    "Geophysical inversion often casts problems in a Bayesian framework, where information on input parameters are represented in probabilistic terms. The solution is given by _a posteriori_ probability, $P(\\mathbf{m}|\\mathbf{d})$, which is proportional to the product of the likelihood function $P(\\mathbf{d}|\\mathbf{m})$ and the _a priori_ probability $P(\\mathbf{m})$,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{m}|\\mathbf{d}) \\propto P(\\mathbf{d}|\\mathbf{m}) \\cdot P(\\mathbf{m})\n",
    "$$\n",
    "\n",
    "The likelihood function is the probability of reproducing the data $\\mathbf{d}$ given a particular model $\\mathbf{m}$, and the _a priori_ model is what we know about the model before assimilating the data. The posterior probability can be evaluated through an objective function, $S(\\mathbf{m})$, which jointly compares the misfit between data and prior information, $\\mathbf{m}_p$,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{m}|\\mathbf{d}) = A \\, \\exp (-S(\\mathbf{m}))\n",
    "$$\n",
    "\n",
    "where $A$ is a constant. We seek the maximum _a posteriori_ (MAP) estimate, which may be obtained by minimising the $\\ell_p$-norm objective function if data and prior information are both uncorrelated,\n",
    "\n",
    "$$\n",
    "S(\\mathbf{m}) = \\frac{1}{s} \\sum_{i} \\frac{\\vert g^i(\\mathbf{m}) - d^i \\vert^s}{(\\sigma^i_d)^s} + \\frac{1}{r} \\sum_{j} \\frac{\\vert \\mathrm{m}^j - \\mathrm{m}^j_p \\vert^r}{(\\sigma^j_p)^r}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{g}(\\mathbf{m})$ is the forward operator that forms an explicit link with the data. In our case $\\mathbf{g}(\\mathbf{m}) = \\Phi$ and the objective function quantifies how well the analytic expression for the power spectrum, computed from model parameters $\\Phi(\\mathbf{m})$, fits the power spectrum computed from the magnetic anomaly, $\\Phi_d$,\n",
    "\n",
    "$$\n",
    "S(\\mathbf{m}) = \\frac{1}{s} \\sum_i \\frac{\\lvert \\Phi^i(\\mathbf{m}) - \\Phi_d^i \\lvert^s}{(\\sigma_{\\Phi}^i)^s} + \\frac{1}{r} \\sum_{j} \\frac{\\vert \\mathrm{m}^j - \\mathrm{m}^j_p \\vert^r}{(\\sigma^j_p)^r}\n",
    "$$\n",
    "\n",
    "By default, PyCurious assumes Gaussian uncertainties on $\\Phi$ ($s=2$), and a uniform prior over a large range ($r=\\infty$) for each model parameter where $-\\sigma_p^j \\leq \\mathrm{m}^j - \\mathrm{m}_p^j \\leq +\\sigma_p^j$. The prior can easily be defined by the user as we will demonstrate later. We have already covered the `optimise` method that uses gradient-based optimisation to quickly iterate to the MAP estimate in [Ex2-Compute-Curie-depth](./Ex2-Compute-Curie-depth.ipynb).\n",
    "\n",
    "\n",
    "> **Note:** The objective function can be accessed (and modified) under `grid.objective_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta_opt, zt_opt, dz_opt, C_opt = grid.optimise(window_size, xpt, ypt, taper=None)\n",
    "\n",
    "k, Phi, sigma_Phi = grid.radial_spectrum(subgrid, taper=None)\n",
    "\n",
    "# define a range of parameters over which to analyse the misfit\n",
    "# use the optimum parameters to help constrain an appropriate range\n",
    "nb = 50\n",
    "beta_range = np.linspace(beta_opt - 0.5, beta_opt + 0.5,nb)\n",
    "dz_range = np.linspace(dz_opt - 5.0, dz_opt + 5.0, nb)\n",
    "\n",
    "# constant values for zt and C\n",
    "zt = zt_opt\n",
    "C = C_opt\n",
    "\n",
    "# misfit matrix\n",
    "misfit = np.zeros((nb,nb))\n",
    "\n",
    "for i, beta in enumerate(beta_range):\n",
    "    for j, dz in enumerate(dz_range):\n",
    "\n",
    "        m = [beta, zt, dz, C]\n",
    "        \n",
    "        misfit[i,j] = grid.min_func(m, k, Phi, sigma_Phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot joint probability\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, sharey=True, figsize=(12,4))\n",
    "\n",
    "X, Y = np.meshgrid(dz_range, beta_range)\n",
    "\n",
    "im1 = ax1.pcolormesh(X, Y, misfit)\n",
    "im2 = ax2.pcolormesh(X, Y, np.exp(-misfit))\n",
    "ax1.scatter(dz_opt, beta_opt, c='r')\n",
    "ax2.scatter(dz_opt, beta_opt, c='r')\n",
    "\n",
    "ax1.set_ylabel(r\"$\\beta$\")\n",
    "ax2.set_ylabel(r\"$\\beta$\")\n",
    "ax1.set_xlabel(r\"$\\Delta z$\")\n",
    "ax2.set_xlabel(r\"$\\Delta z$\")\n",
    "ax1.set_title(r\"misfit $S(\\mathbf{m})$\")\n",
    "ax2.set_title(r\"Posterior probability $P(\\mathbf{m}|\\mathbf{d})$\")\n",
    "\n",
    "fig.colorbar(im1, ax=ax1)\n",
    "fig.colorbar(im2, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings algorithm\n",
    "\n",
    "A practical approach to sample the posterior is to use MCMC methods such as the Metropolis-Hastings algorithm. This implements a random walk where $k=1,2,\\ldots,n$ for a Markov Chain $n$ samples long.\n",
    "\n",
    "1. Generate a candidate $\\mathbf{m}'$ for the next sample by picking from the prior distribution $P(\\mathbf{m})$.\n",
    "2. Calculate the acceptance ratio between each sample of the posterior $\\alpha = P(\\mathbf{m}'|\\mathbf{d})/P(\\mathbf{m}_{k}|\\mathbf{d})$\n",
    "3. Generate a random number $u: [0,1]$ which will be used to decide if a candidate is accepted or rejected.\n",
    "    - accept if $u \\leq \\alpha$ and set $\\mathbf{m}_{k+1} = \\mathbf{m}'$\n",
    "    - reject if $u > \\alpha$ and set $\\mathbf{m}_{k+1} = \\mathbf{m}_k$ instead\n",
    "\n",
    "The random walk should be initialised not too far from the MAP estimate. MCMC methods require tuning to adequately sample the posterior. Parameters to tweak include:\n",
    "\n",
    "- `burnin`: the number of burn-in simulations\n",
    "- `nsim`: total simulation after burnin\n",
    "- `x_scale`: scaling factor $\\gamma$ to generate a new candidate $\\mathbf{m}' = \\gamma \\mathbf{m}$.\n",
    "\n",
    "```python\n",
    "posterior = metropolis_hastings(window_size, xpt, ypt, nsim, burnin, x_scale)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k, Phi, sigma_Phi = grid.radial_spectrum(subgrid, taper=None)\n",
    "\n",
    "x_scale = [0.25, 0.1, 1.0, 1.0]\n",
    "\n",
    "burnin = 1000\n",
    "nsim = 100000\n",
    "\n",
    "# C can be initialised close to its MAP very easily\n",
    "posterior = grid.metropolis_hastings(window_size, xpt, ypt, nsim, burnin, x_scale, C=Phi.mean(), taper=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appraising the ensemble\n",
    "\n",
    "How best to visualise the ensemble after the MCMC simulation? We cover some of the following:\n",
    "\n",
    "- Histograms\n",
    "- Scatter plots & joint probabilities\n",
    "- Trajectory of the random walk\n",
    "- MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [r'$\\beta$', r'$z_t$', r'$\\Delta z$', r'$C$']\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(14,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(posterior[i], bins=20, normed=True)\n",
    "    ax.set_title(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some joint probabilities visualised with the trajectory of the random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot joint probability and trajectory of random walk\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,  figsize=(12,4))\n",
    "\n",
    "\n",
    "H1, xedges, yedges = np.histogram2d(posterior[1], posterior[3], normed=True)\n",
    "X1, Y1 = np.meshgrid(xedges, yedges)\n",
    "\n",
    "H2, xedges, yedges = np.histogram2d(posterior[2], posterior[0], normed=True)\n",
    "X2, Y2 = np.meshgrid(xedges, yedges)\n",
    "\n",
    "\n",
    "ax1.pcolormesh(X1, Y1, H1)\n",
    "ax1.plot(posterior[1], posterior[3], c='w')\n",
    "\n",
    "ax2.pcolormesh(X2, Y2, H2)\n",
    "ax2.plot(posterior[2], posterior[0], c='w')\n",
    "\n",
    "ax1.set_xlabel(r\"$z_t$\")\n",
    "ax1.set_ylabel(r\"$C$\")\n",
    "ax2.set_xlabel(r\"$\\Delta z$\")\n",
    "ax2.set_ylabel(r\"$\\beta$\")\n",
    "\n",
    "fig.colorbar(im1, ax=ax1)\n",
    "fig.colorbar(im2, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_result = stats.mode(np.vstack(posterior).T, axis=0)\n",
    "\n",
    "x_opt1 = grid.optimise(window_size, xpt, ypt, taper=None)\n",
    "x_opt2 = mode_result.mode.ravel()\n",
    "\n",
    "str_fmt = \"beta = {:.2f}  zt = {:.2f}  dz = {:5.2f}  C = {:.2f}\"\n",
    "\n",
    "print(str_fmt.format(*x_opt1))\n",
    "print(str_fmt.format(*x_opt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,sharey=True,figsize=(12,4))\n",
    "\n",
    "Phi_opt1 = pycurious.bouligand2009(k, *x_opt1)\n",
    "Phi_opt2 = pycurious.bouligand2009(k, *x_opt2)\n",
    "\n",
    "ax1.plot(k, Phi, 'o', c='0.5')\n",
    "ax1.plot(k, Phi_opt1, linewidth=2, label='MAP (optimised)')\n",
    "ax1.plot(k, Phi_opt2, linewidth=2, label='MAP (MCMC)')\n",
    "ax2.semilogx(k, Phi, 'o', c='0.5')\n",
    "ax2.semilogx(k, Phi_opt1, linewidth=2, label='MAP (optimised)')\n",
    "ax2.semilogx(k, Phi_opt2, linewidth=2, label='MAP (MCMC)')\n",
    "# ax2.semilogx(k, S2)\n",
    "\n",
    "ax1.set_ylabel('radial power spectrum')\n",
    "ax1.set_xlabel('wavenumber (rad/km)')\n",
    "ax2.set_xlabel('log wavenumber (rad/km)')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP estimate obtained from `grid.optimise` should be identical to that from `grid.metropolis_hastings`, but may in differ in practice if `nsim` is too few or `x_scale` is not properly tuned. Running multiple chains in parallel should alleviate most concerns. Otherwise parts of the probability space may be non-unique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
